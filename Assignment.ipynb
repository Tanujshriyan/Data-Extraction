{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import os, os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tanujshriyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tanujshriyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data_df =  pd.read_excel('Input.xlsx')\n",
    "excel_data_df.head()\n",
    "links = excel_data_df['URL']\n",
    "URlID = excel_data_df['URL_ID']\n",
    "print(f'Total {len(links)} reports found')\n",
    "print(f'Total {len(URlID)} reports found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "\n",
    "i = 0\n",
    "for url in links:\n",
    "        r = requests.get(url, headers=headers)\n",
    "        data = r.text\n",
    "        soup = BeautifulSoup(data, 'html.parser')\n",
    "        #print(soup)\n",
    "        for each in ['h1']:\n",
    "            s = soup.find(each)\n",
    "            #print(p)\n",
    "            f = open(f'Textfiles/{URlID[i]}.txt', 'w+')\n",
    "            f.write('Title: '+s.extract().text)\n",
    "            f.close() \n",
    "        for data in soup.find_all(\"p\"):\n",
    "            f = open(f'Textfiles/{URlID[i]}.txt', 'a')\n",
    "            f.write('\\n'+ data.get_text())\n",
    "            f.close() \n",
    "        i=i+1 \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Stop Words are 14108\n"
     ]
    }
   ],
   "source": [
    "with open('StopWords/StopWords_Generic.txt','r') as f:\n",
    "    stop_words_generic = f.read()\n",
    "\n",
    "stop_words_generic = stop_words_generic.split('\\n')\n",
    "print(f'Total number of Stop Words are {len(stop_words_generic)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Positive Words are 2007\n"
     ]
    }
   ],
   "source": [
    "with open('MasterDictionary/positive-words.txt','r') as f:\n",
    "    positive_words = f.read()\n",
    "\n",
    "positive_words = positive_words.split('\\n')\n",
    "print(f'Total number of Positive Words are {len(positive_words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xef in position 28564: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m/Users/tanujshriyan/Desktop/Projects/Python/Data-Extraction/Assignment.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tanujshriyan/Desktop/Projects/Python/Data-Extraction/Assignment.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mMasterDictionary/negative-words.txt\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tanujshriyan/Desktop/Projects/Python/Data-Extraction/Assignment.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     negative_words \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39;49mread()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tanujshriyan/Desktop/Projects/Python/Data-Extraction/Assignment.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m negative_words \u001b[39m=\u001b[39m negative_words\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tanujshriyan/Desktop/Projects/Python/Data-Extraction/Assignment.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTotal number of Negative Words are \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(negative_words)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[39m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m+\u001b[39m \u001b[39minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m     (result, consumed) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_buffer_decode(data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors, final)\n\u001b[1;32m    323\u001b[0m     \u001b[39m# keep undecoded input until the next call\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m=\u001b[39m data[consumed:]\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xef in position 28564: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "with open('MasterDictionary/negative-words.txt','r') as f:\n",
    "    negative_words = f.read()\n",
    "\n",
    "negative_words = negative_words.split('\\n')\n",
    "print(f'Total number of Negative Words are {len(negative_words)}')\n",
    "print(negative_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = re.sub(r'[^A-Za-z]','',text.upper())\n",
    "    tokenized_words = word_tokenize(text)\n",
    "    return tokenized_words\n",
    "\n",
    "def tokenize(text):\n",
    "    text = re.sub(r'[^A-Za-z]',' ',text.upper())\n",
    "    tokenized_words = word_tokenize(text)\n",
    "    return tokenized_words\n",
    "\n",
    "def remove_stopwords(words, stop_words):\n",
    "    return [x for x in words if x not in stop_words]\n",
    "    \n",
    "def countfunc(store, words):\n",
    "    score = 0\n",
    "    for x in words:\n",
    "        if(x in store):\n",
    "            score = score+1\n",
    "    return score\n",
    "\n",
    "def sentiment(score):\n",
    "    if(score < -0.5):\n",
    "        return 'Most Negative'\n",
    "    elif(score >= -0.5 and score < 0):\n",
    "        return 'Negative'\n",
    "    elif(score == 0):\n",
    "        return 'Neutral'\n",
    "    elif(score > 0 and score < 0.5):\n",
    "        return 'Positive'\n",
    "    else:\n",
    "        return 'Very Positive'\n",
    "    \n",
    "\n",
    "def polarity(positive_score, negative_score):\n",
    "    return (positive_score - negative_score)/((positive_score + negative_score)+ 0.000001)\n",
    "     \n",
    "\n",
    "def subjectivity(positive_score, negative_score, num_words):\n",
    "    return (positive_score+negative_score)/(num_words+ 0.000001)\n",
    "\n",
    "def syllable_morethan2(word):\n",
    "    if(len(word) > 2 and (word[-2:] == 'es' or word[-2:] == 'ed')):\n",
    "        return False\n",
    "    \n",
    "    count =0\n",
    "    vowels = ['a','e','i','o','u']\n",
    "    for i in word:\n",
    "        if(i.lower() in vowels):\n",
    "            count = count +1\n",
    "        \n",
    "    if(count > 2):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def fog_index_cal(average_sentence_length, percentage_complexwords):\n",
    "    return 0.4*(average_sentence_length + percentage_complexwords)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = ['positive_score',\n",
    "      'negative_score',\n",
    "      'polarity_score',\n",
    "      'subjectivity_score',\n",
    "      'average_sentence_length',\n",
    "      'percentage_of_complex_words',\n",
    "      'fog_index',\n",
    "      'avg_number_of_words_per_sentence',\n",
    "      'complex_word_count',\n",
    "      'word_count',\n",
    "      'syllable_count',\n",
    "      'personal_pronouns',\n",
    "      'avg_word_length']\n",
    "\n",
    "for v in var:\n",
    "    excel_data_df[v] = 0\n",
    "    \n",
    "excel_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,len(URlID)):\n",
    "    with open(f'{i}.txt', 'r') as f:\n",
    "        x = f.read()\n",
    "        \n",
    "        if x:\n",
    "            start, end = 0, len(x)\n",
    "            content = x[start:end] \n",
    "            if ('...' not in content) and ('. . .' not in content) and len(content) > 200:\n",
    "                tokenized_words = tokenize(content) \n",
    "                #print(f'Total tokenized words are {len(tokenized_words)}')\n",
    "                \n",
    "                words = remove_stopwords(tokenized_words, stop_words_generic)\n",
    "                num_words = len(words)\n",
    "                #print(f'Total words after removing stop words are {len(words)}')\n",
    "                \n",
    "                positive_score = countfunc(positive_words,words)\n",
    "                negative_score = countfunc(negative_words, words)\n",
    "                #print(f'Total positive score is {positive_score}')\n",
    "                #print(f'Total negative score is {negative_score}')\n",
    "                \n",
    "                polarity_score = polarity(positive_score, negative_score)\n",
    "                #print(polarity_score)\n",
    "                \n",
    "                subjectivity_score = subjectivity(positive_score, negative_score, num_words)\n",
    "                #print(subjectivity_score)\n",
    "                #print(sentiment(polarity_score))\n",
    "                \n",
    "                sentences = sent_tokenize(content)\n",
    "                num_sentences = len(sentences)\n",
    "                average_sentence_length = num_words/num_sentences   \n",
    "        \n",
    "                num_complexword = 0\n",
    "                \n",
    "                for word in words:\n",
    "                    if(syllable_morethan2(word)):\n",
    "                        num_complexword = num_complexword+1\n",
    "                        \n",
    "                #print(num_complexword)\n",
    "                percentage_complexwords = num_complexword/num_words\n",
    "                #print(percentage_complexwords)\n",
    "                fog_index = fog_index_cal(average_sentence_length, percentage_complexwords)\n",
    "                #print(fog_index)\n",
    "                \n",
    "                positive_word_proportion = positive_score/num_words\n",
    "                negative_word_proportion = negative_score/num_words\n",
    "                \n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
